{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Goal-Oriented Chatbot with Deep Reinforcement Learning\n",
    "在本系列教程中我们将学会如何从零开始搭建一个基于python深度强化学习的任务型对话机器人系统，代码见[here](https://github.com/maxbren/GO-Bot-DRL)\n",
    "### 内容提纲\n",
    "* 简介和训练流程\n",
    "* DQNAgent\n",
    "* 对话状态追踪器\n",
    "* 用户仿真和错误控制\n",
    "* 运行和Future work\n",
    "\n",
    "## What is a Goal-Oriented Chatbot?\n",
    "一个任务型机器人是用于帮助用户解决特定方面的问题，例如订机票，查订单等。实现一个任务型机器人主要包括两种方法：1.使用encoder-decoder类的监督学习，直接将对话映射到答案；2. 使用强化学习，通过真实用户或者仿真用户进行试错对话。使用强化学习训练对话机器人技术上已经非常成熟。\n",
    "### Dialog System\n",
    "基于强化学习的对话系统一般分为三部分：\n",
    "1. Dialogue Manager (DM)\n",
    "    * Dialogue State Tracker (DST) / State Tracker (ST)\n",
    "    * 针对agent的policy, 使用NN实现\n",
    "2. Natural Language Understanding (NLU) unit\n",
    "3. Natural Language Generator (NLG) unit.\n",
    "此外，系统流程中还有一个带有目的的用户，这个目的即用户想要从对话中完成什么任务。例如下图中的订餐对话流程图\n",
    "![avatar](img/dialog-flow.png)<br>\n",
    "在这个流程中，用户的表述被NLU模块转化为可以被`agent`处理的底层语义框架，之后DST模块将当前语义框架结合历史对话，转化成可以被agent策略使用的状态`state`表达。这个state将作为agent的输入，最终输出一个行为`action`。agent也可以从数据库中获取任务需要的其他信息。最后agent的action将被NLG模块转化为自然语言。\n",
    "![avatar](img/TC-Bot.png)<br>\n",
    "本教程及代码时基于MluLAB的对话系统，称为[TC-Bot](https://github.com/MiuLab/TC-Bot)，其论文的主要介绍了如何实现一个仿真用户并将其应用于训练。我们的论文特色是提供了完整代码，介绍如何训练。\n",
    "### 用户模拟器和错误模型控制器\n",
    "用户仿真是一种确定的基于规则的控制器，它基于用户进程建模，使用一个内部状态来表示用户sim的约束和需求。这个内部状态持续追踪当前对话和完成目标所需要的做的事。目标是从可用的用户目标列表中随机挑选出来的，其中目标由一组约束和其他信息组成，在用户sim试图实现当前目标时指导其操作。采用误差模型控制器在语义框架的层次上向用户sim的动作添加误差，提高了训练的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Ticket Data \n",
    "1. `Database`: 电影票的database是带有不同插槽`slots`的电影票数据。ticket数据以dict形式展示，key表示index，value包含了票的信息，每种票的信息基本各不相同。\n",
    "2. `Database Dictionary`: 提供了每个`slots`的可选项\n",
    "3. `User Goal List` 我们将用户目标作为一个字典列表，其中包含每个目标的请求和通知槽。使用这个数据库的是为了让agent找到基于这些user goal下的符合要求的电影票，由于每张电影票是唯一的且多数电影票的slots不同，这个任务不是很简单。\n",
    "### Anatomy of an Action \n",
    "了解该系统中行为的结构。用户sim和agent都将语义框架格式作为输入和输出，一个行为包括一个意图`intent`，通知槽和请求槽。`Slot`在这是指能代表一个通知或请求的key-value对。如```{'starttime': 'tonight', 'theater': 'regal 16'}, 'starttime': 'tonight' and 'theater': 'regal 16'```都是插槽。<br>\n",
    "意图`intent`表示行为的类型。action分为告知(`inform`)类型，表示限制条件，告知slots中包含发送者想要接受者知道的信息，请求(`request`)类型，表示发送者想要从接受者处获得的信息，因此是以```{key:Unknown}```的形式(这里sender指售票方，receiver指订票方)。\n",
    "\n",
    "### All intents\n",
    "* `Inform` 表示限制条件\n",
    "* `Request` 表示待填信息\n",
    "* `Thanks` 用户(购票者)表示感谢\n",
    "* `Match Found` 仅由agent表示，指示用户它拥有一个它认为可以实现用户目标的匹配项\n",
    "* `Reject` 仅由用户表示，用于回应agent的match found行为不满足限制条件\n",
    "* `Done` agent使用它来关闭对话并查看它是否完成了当前的目标，当流程进行得太久时，用户动作会自动有这个意图\n",
    "\n",
    "### State\n",
    "状态由ST创建，作为代理选择适当操作的输入。它是来自当前会话历史的大量有用信息的数组。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Agent\n",
    "![avatar](\"img/train-loop.png\")\n",
    "该图表示了模型训练中一个完整的循环，四个主要部分是agent`dqn_agent`, dialog state tracker`state_tracker`, user(or user simultor) `user` 和Error Model Controller `EMC`，系统的工作流程如下：\n",
    "1. 获得当前状态并将其作为输入发送给agent的Get action方法\n",
    "    1. 从上一状态获得，当前状态=上一个\"next_state\"\n",
    "    2. 如果这是情节的开始，则获取初始状态\n",
    "2. 获得agent的行动，并将其发送给state tracker的更新方法, ST将在这个方法中更新当前对话的历史记录以及根据数据库信息更新agent的行动\n",
    "3. 更新后的agent行动将作为用户step方法的输入，在step方法中，用户sim会创建自己的基于规则的响应，并输出奖励和成功信息\n",
    "4. EMC向用户操作注入了错误\n",
    "5. 带有error的用户行动作为输入发送到ST更新方法，与ST更新agent action的方法类似，但是，它仅将信息保存在其历史记录中，而不以重要方式更新user action.\n",
    "6. 最后，从ST中获得状态作为\"next_state\"输出，这就完成了agent在当前轮的经验元组，并添加到agent的记忆中了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_round(state, warmup=False):\n",
    "    # 1) Agent takes action given state tracker's representation of dialogue (state)\n",
    "    agent_action_index, agent_action = dqn_agent.get_action(state, use_rule=True)\n",
    "    # 2) Update state tracker with the agent's action\n",
    "    round_num = state_tracker.update_state_action(agent_action)\n",
    "    # 3) User takes action given agent action\n",
    "    user_action, reward, done, success = user.step(agent_action, round_num)\n",
    "    if not done:\n",
    "        # 4) Infuse error into semantic frame level of user action\n",
    "        emc.infuse_error(user_action)\n",
    "    # 5) Update state tracker with user action\n",
    "    state_tracker.update_state_user(user_action)\n",
    "    # 6) Get next state and add experience\n",
    "    next_state = state_tracker.get_state(done)\n",
    "    dqn_agent.add_experience(state, agent_action_index, reward, next_state, done)\n",
    "    \n",
    "    return next_state, reward, next_state, success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，与任何DQN代理一样，内存缓冲区在预热阶段会在一定程度上被填充。与游戏中DQNs的许多使用不同，代理在这一阶段不会采取随机行动。相反，在热身过程中，它使用一个非常简单的基于规则的算法，这将在第二部分中解释。  \n",
    "在整个流程中并没有使用自然语言模块，NLG和NLU模块在预训练中使用，且与agent的训练独立开。\n",
    "### Episode Reset\n",
    "在warm-up和training loop之前有重置函数，在每个episode前调用（本文中的episode=conversation）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_reset():\n",
    "    # First reset the state tracker\n",
    "    state_tracker.reset()\n",
    "    # Then pick an init user action\n",
    "    user_action = user.reset()\n",
    "    # Infuse with error\n",
    "    emc.infuse_error()\n",
    "    # Add update state tracker\n",
    "    state_tracker = update_state_user(user_action)\n",
    "    # Finally, reset agent\n",
    "    dqn_agent.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_run():\n",
    "    total_step = 0\n",
    "    while total_step != WARMUP_MEM and not dqn_agent.is_memory_full():\n",
    "        # Reset episode\n",
    "        episode_reset()\n",
    "        done = False\n",
    "        # Get initial state from state tracker\n",
    "        state = state_tracker.get_state()\n",
    "        while not done:\n",
    "            next_state, _, done, _ = run_round(state, warmup=True)\n",
    "            total_step += 1\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们定义外部循环，只运行到代理的内存被填满以预热MEM或其内存缓冲区完全被填满为止。接下来重置episode，获得初始状态。内循环执行`run_round`直到`done == True`，表示episode执行完毕。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_run():\n",
    "    episode = 0\n",
    "    period_success_total = 0\n",
    "    success_rate_best = 0\n",
    "    # Almost exact same loop as warm-up\n",
    "    while episode < NUM_EP_TRAIN:\n",
    "        episode_reset()\n",
    "        episode += 1\n",
    "        done = False\n",
    "        state = state_tracker.get_state()\n",
    "        while not done:\n",
    "            next_state, reward, done, success = run_round(state)\n",
    "            period_reward_total += reward\n",
    "            state = next_state\n",
    "        # ------\n",
    "    period_success_total += success\n",
    "    # Train block\n",
    "    if episode % TRAIN_FREQ == 0:\n",
    "        # Get success rate\n",
    "        success_rate = period_success_total / TRAIN_FREQ\n",
    "        # 1. Empty memory buffer if statemement is true\n",
    "        if success_rate >= success_rate_best and success_rate >= SUCCESS_RATE_THRESHOLD:\n",
    "            dqn_agent.empty_memory()\n",
    "            success_rate_best = success_rate\n",
    "        # Refresh period success total\n",
    "        period_success_total = 0\n",
    "        # 2. Copy weights\n",
    "        dqn_agent.copy()\n",
    "        # 3. Train weights\n",
    "        dqn_agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "忽略一些额外的变量，loop与warm-up非常相似。到目前为止，主要的区别在于，当剧集数量达到`NUM_EP_TRAIN`时，该方法将结束其外部循环。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinghao",
   "language": "python",
   "name": "jinghao"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
